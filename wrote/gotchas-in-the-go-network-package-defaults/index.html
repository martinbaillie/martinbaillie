<!doctype html><html><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><script>if(localStorage.theme!="")document.documentElement.setAttribute("data-theme",localStorage.theme)</script><link rel="stylesheet preload prefetch" as=style type=text/css media=screen href=https://martin.baillie.id/main.60aec2f2e6c07375dfe74807e7dc14a15f8cb0e9d5dbca91bc05311d6e58627e1aef795f341332a861be094a5e5fc8c3c249d63994f5b77f661cd03d56645731.css integrity="sha512-YK7C8ubAc3Xf50gH59wUoV+MsOnV28qRvAUxHW5YYn4a73lfNBMyqGG+CUpeX8jDwknWOZT1t39mHNA9VmRXMQ==" crossorigin=anonymous><title>Martin Baillie | Gotchas in the Go Network Package Defaults</title><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><link rel=canonical href=https://martin.baillie.id/wrote/gotchas-in-the-go-network-package-defaults/><meta property="og:title" content="Gotchas in the Go Network Package Defaults"><meta name=description content="Fool Me Once I have been keeping a wee .org file of Go net gotchas for a while now and I pull it up each time I&rsquo;m building a service with the standard library, just to make sure I don&rsquo;t miss something basic that I&rsquo;ve hit in the past. Let&rsquo;s call it learning from one&rsquo;s mistakes where the &ldquo;one&rdquo; in question has a shocking memory.
I&rsquo;ve just this week found myself adding another entry after a production incident and thought there was enough in there to merit tidying up and posting."><meta property="og:image" content="https://martin.baillie.id//img/me.jpg"><meta itemprop=name content="Gotchas in the Go Network Package Defaults"><meta name=application-name content="Martin Baillie"><meta property="og:site_name" content="Martin Baillie"><meta property="og:title" content="Gotchas in the Go Network Package Defaults"><meta property="og:description" content="Fool Me Once I have been keeping a wee .org file of Go net gotchas for a while now and I pull it up each time I&rsquo;m building a service with the standard library, just to make sure I don&rsquo;t miss something basic that I&rsquo;ve hit in the past. Let&rsquo;s call it learning from one&rsquo;s mistakes where the &ldquo;one&rdquo; in question has a shocking memory.
I&rsquo;ve just this week found myself adding another entry after a production incident and thought there was enough in there to merit tidying up and posting."><meta property="og:type" content="article"><meta property="og:url" content="https://martin.baillie.id/wrote/gotchas-in-the-go-network-package-defaults/"><meta property="article:published_time" content="2021-03-21T21:19:00+11:00"><meta property="article:modified_time" content="2021-03-21T21:19:00+11:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Gotchas in the Go Network Package Defaults"><meta name=twitter:description content="Fool Me Once I have been keeping a wee .org file of Go net gotchas for a while now and I pull it up each time I&rsquo;m building a service with the standard library, just to make sure I don&rsquo;t miss something basic that I&rsquo;ve hit in the past. Let&rsquo;s call it learning from one&rsquo;s mistakes where the &ldquo;one&rdquo; in question has a shocking memory.
I&rsquo;ve just this week found myself adding another entry after a production incident and thought there was enough in there to merit tidying up and posting."><meta name=twitter:site content="@martinbaillie"><meta name=twitter:creator content="@martinbaillie"><meta name=twitter:image content="https://martin.baillie.id//img/me.jpg"><link rel=authorization_endpoint href=https://indieauth.com/auth><link rel=token_endpoint href=https://tokens.indieauth.com/token><link rel=webmention href=https://webmention.io/martin.baillie.id/webmention><link rel=pingback href=https://webmention.io/martin.baillie.id/xmlrpc><link rel=apple-touch-icon sizes=57x57 href=/img/favicon/apple-icon-57x57.png><link rel=apple-touch-icon sizes=60x60 href=/img/favicon/apple-icon-60x60.png><link rel=apple-touch-icon sizes=72x72 href=/img/favicon/apple-icon-72x72.png><link rel=apple-touch-icon sizes=76x76 href=/img/favicon/apple-icon-76x76.png><link rel=apple-touch-icon sizes=114x114 href=/img/favicon/apple-icon-114x114.png><link rel=apple-touch-icon sizes=120x120 href=/img/favicon/apple-icon-120x120.png><link rel=apple-touch-icon sizes=144x144 href=/img/favicon/apple-icon-144x144.png><link rel=apple-touch-icon sizes=152x152 href=/img/favicon/apple-icon-152x152.png><link rel=apple-touch-icon sizes=180x180 href=/img/favicon/apple-icon-180x180.png><link rel=icon type=image/png sizes=192x192 href=/img/favicon/android-icon-192x192.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon/favicon-32x32.png><link rel=icon type=image/png sizes=96x96 href=/img/favicon/favicon-96x96.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon/favicon-16x16.png><link rel=manifest href=/img/favicon/manifest.json><meta name=msapplication-TileImage content="/img/favicon/ms-icon-144x144.png"><meta name=msapplication-TileColor content="#ffffff"><meta name=theme-color content="#ffffff"><script src=https://martin.baillie.id/main.2886ed11170936a33d9c58c04cac23204ead8d0a4e228982a00ba971f39d3b391b3cdc6dadb283cdc42af9286068e8901d6998bff909b9edb6482bc2629ebb92.js integrity="sha512-KIbtERcJNqM9nFjATKwjIE6tjQpOIomCoAupcfOdOzkbPNxtrbKDzcQq+ShgaOiQHWmYv/kJue22SCvCYp67kg==" crossorigin=anonymous async></script></head><body><div class=content><header><div class=main><a href=https://martin.baillie.id/>Martin Baillie</a></div><nav><a class=soc id=tags href=/tags title=Tags><i data-feather=tag></i></a><a class=soc id=feed href=/index.xml title="RSS Feed"><i data-feather=rss></i></a><a class=soc id=mode style=cursor:pointer onclick=toggleMode(); title="Switch Theme"><i data-feather=moon></i></a></nav></header><main><article><div class=title><h1>Gotchas in the Go Network Package Defaults</h1></div><div class=meta><div class=posted><i data-feather=edit-3></i>20210321</div><div class=reading><i data-feather=clock></i>~25 mins</div></div><div class=tldr><strong>tl;dr:</strong>
Things I keep forgetting about Go's network package defaults.</div><section class=body><div class=single><h2 id=fool-me-once>Fool Me Once</h2><p>I have been keeping a wee <code>.org</code> file of Go <code>net</code> gotchas for a while now and I
pull it up each time I&rsquo;m building a service with the standard library, just to
make sure I don&rsquo;t miss something basic that I&rsquo;ve hit in the past. Let&rsquo;s call it
learning from one&rsquo;s mistakes where the <em>&ldquo;one&rdquo;</em> in question has a shocking
memory.</p><p>I&rsquo;ve just this week found myself adding another entry after a production
incident and thought there was enough in there to merit tidying up and posting.</p><p>Well, it has been a <a href=https://en.wikipedia.org/wiki/Severe%5Fstorm%5Fevents%5Fin%5FSydney#2020s%E2%80%93present>very soggy</a> Sunday in Sydney so here is the current list in
all its glory, with some added justification for good measure. Just bear in mind
that they mostly pertain to HTTP clients and servers from using Go at (relative)
scale in service oriented architectures. Also, I think they are all still
relevant as of Go 1.16 but happy to be corrected on that.</p><p>So without further ado, top of the file is:</p><h2 id=timeouts>Timeouts</h2><p>Set them! The network is <a href="https://queue.acm.org/detail.cfm?id=2655736">unreliable</a> and the standard library default clients and
servers do not set their main timeouts, and all of them interpret the zero value
as infinity to boot. Timeouts are subjective to the use case and the Go core
team have steered clear of making any sweeping generalisations.</p><blockquote><p>NOTE: This includes all use of the package level convenience functions too:
<code>http.Get</code> and client friends, <code>http.ListenAndServe</code> and server friends.</p></blockquote><p>A corollary to this is you should practically <strong>always</strong> have a customised
<code>http.Client</code> and/or <code>http.Server</code> in a production Go service.</p><h3 id=client-timeouts>Client timeouts</h3><p>For clients you often only need to configure the main timeout (zero value by
default). It covers the E2E exchange and is most likely how your mental model of
an RPC works:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>c</span> <span class=o>:=</span> <span class=o>&amp;</span><span class=nx>http</span><span class=p>.</span><span class=nx>Client</span><span class=p>{</span>
	<span class=nx>Timeout</span><span class=p>:</span> <span class=mi>5</span> <span class=o>*</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Second</span><span class=p>,</span>
<span class=p>}</span>
</code></pre></div><p>This timeout includes any HTTP <code>3xx</code> redirect durations, the reading of response
body and the connection and handshake times (unless a reused connection). I find I am usually done here regarding clients.</p><p>However, for granular control over these individual properties and more, you
need to drop lower to the underlying transport:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>c</span> <span class=o>:=</span> <span class=o>&amp;</span><span class=nx>http</span><span class=p>.</span><span class=nx>Client</span><span class=p>{</span>
	<span class=nx>Timeout</span><span class=p>:</span> <span class=mi>5</span> <span class=o>*</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Second</span><span class=p>,</span>
	<span class=nx>Transport</span><span class=p>:</span> <span class=o>&amp;</span><span class=nx>http</span><span class=p>.</span><span class=nx>Transport</span><span class=p>{</span>
		<span class=nx>DialContext</span><span class=p>:</span> <span class=p>(</span><span class=o>&amp;</span><span class=nx>net</span><span class=p>.</span><span class=nx>Dialer</span><span class=p>{</span>
			<span class=c1>// This is the TCP connect timeout in this instance.
</span><span class=c1></span>			<span class=nx>Timeout</span><span class=p>:</span> <span class=mi>2500</span> <span class=o>*</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Millisecond</span><span class=p>,</span>
		<span class=p>}).</span><span class=nx>DialContext</span><span class=p>,</span>
		<span class=nx>TLSHandshakeTimeout</span><span class=p>:</span> <span class=mi>2500</span> <span class=o>*</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Millisecond</span><span class=p>,</span>
	<span class=p>},</span>
<span class=p>}</span>
</code></pre></div><blockquote><p>NOTE: Since response bodies are read after the client method has returned you
need to use a <code>time.Timer</code> to take granular control over their acceptable read
times.</p></blockquote><p>There are more timeouts on the transport that I have never had a need for such
as the <code>ResponseHeaderTimeout</code> (time to wait for response headers after request
writing ends) and the <code>ExpectContinueTimeout</code> (time to wait for a <code>100-Continue</code>
if using HTTP Expect headers).</p><p>There are also settings related to reuse, such as the transport&rsquo;s
<code>IdleConnTimeout</code> and dialer&rsquo;s <code>KeepAlive</code> settings. These are deserved of their
own <a href=#connection-pooling>section</a>.</p><h3 id=server-timeouts>Server timeouts</h3><p>In the same vein as you not wanting a server to hold your client&rsquo;s requests
hostage because they have no timeout, when writing a Go HTTP server you have the
inverse consideration: you don&rsquo;t want badly behaving or laggy clients holding
your server&rsquo;s file descriptors hostage.</p><p>To avoid this, you should always have a customised <code>http.Server</code> instance:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>s</span> <span class=o>:=</span> <span class=o>&amp;</span><span class=nx>http</span><span class=p>.</span><span class=nx>Server</span><span class=p>{</span>
	<span class=nx>ReadTimeout</span><span class=p>:</span>  <span class=mi>2500</span> <span class=o>*</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Millisecond</span><span class=p>,</span>
	<span class=nx>WriteTimeout</span><span class=p>:</span> <span class=mi>5</span> <span class=o>*</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Second</span><span class=p>,</span>
<span class=p>}</span>
</code></pre></div><p><code>ReadTimeout</code> here covers the time taken to read the request headers and
optionally body, and <code>WriteTimeout</code> covers the duration to the end of the
response write.</p><p>However, if the server is processing TLS then the <code>WriteTimeout</code> ticker actually
starts as soon as that first byte of the TLS handshake is read. In practice this
means you should factor in the whole <code>ReadTimeout</code> and then whatever you want to
accept for writes on top of that.</p><p>Similar to the main <code>http.Client.Timeout</code> value, these are the two main server
timeouts that you should think about appropriate situational values for but
there are a few others that give more granular control (such as the time to read
and write headers respectively). Again, I have never had a need to use them.</p><hr><p>These timeouts cover poorly behaving clients. But with a server, you also should
have a think about how long you are willing to accept as a <strong>request handling
duration</strong>. I mention mental models of client timeouts above; I would argue this
is the server-side version that intuitively springs to mind when you think:
<em>&ldquo;server timeout&rdquo;</em>.</p><p>With Go&rsquo;s <code>http.Server</code> you could implement these timeouts in the handler funcs
themselves. You could also use the <code>TimeoutHandler</code> helper wrapper:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=kd>func</span> <span class=nf>TimeoutHandler</span><span class=p>(</span><span class=nx>h</span> <span class=nx>Handler</span><span class=p>,</span> <span class=nx>dt</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Duration</span><span class=p>,</span> <span class=nx>msg</span> <span class=kt>string</span><span class=p>)</span> <span class=nx>Handler</span>
</code></pre></div><p>Wrapping with this means things are all business-as-usual until <code>dt</code> is breached
at which point a 503 is written down the pipe to the client with the optional
body <code>msg</code>.</p><h2 id=close-http-response-bodies>Close HTTP Response Bodies</h2><p>As a client you may not care about the response body or you may be anticipating
an empty response. Either way, you should close it off. The standard library
will not do it on your behalf and this can hold up connections in the client&rsquo;s
pool preventing reuse (i.e. if using HTTP/1.x keep-alives) or worse, exhaust
host file handles.</p><p>The standard library <em>does</em> guarantee response bodies to be non-nil even in the
cases of responses sans body or zero-length body. So, to close things out safely
the following suffices:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>res</span><span class=p>,</span> <span class=nx>err</span> <span class=o>:=</span> <span class=nx>client</span><span class=p>.</span><span class=nf>Do</span><span class=p>(</span><span class=nx>req</span><span class=p>)</span>
<span class=k>if</span> <span class=nx>err</span> <span class=o>!=</span> <span class=kc>nil</span> <span class=p>{</span>
	<span class=k>return</span> <span class=nx>err</span>
<span class=p>}</span>
<span class=k>defer</span> <span class=nx>res</span><span class=p>.</span><span class=nx>Body</span><span class=p>.</span><span class=nf>Close</span><span class=p>()</span>
<span class=o>...</span>
</code></pre></div><p>If you are not going to do anything with the body then it is still important to
read it to completion. To not do so affects the propensity for reuse,
particularly if the server is pushing a lot of data. Flush the body with:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>_</span><span class=p>,</span> <span class=nx>err</span> <span class=o>:=</span> <span class=nx>io</span><span class=p>.</span><span class=nf>Copy</span><span class=p>(</span><span class=nx>ioutil</span><span class=p>.</span><span class=nx>Discard</span><span class=p>,</span> <span class=nx>res</span><span class=p>.</span><span class=nx>Body</span><span class=p>)</span>
</code></pre></div><blockquote><p>NOTE: Depending on the scenario, it might be pertinent to make an attempt to
reuse the connection, but then more efficient to close it if the server is
pushing a lot of data. <code>io.LimitedReader</code> can help here.</p></blockquote><h2 id=http-1-dot-x-keep-alives>HTTP/1.x Keep-alives</h2><p>Speaking of reuse, keep-alives are Go&rsquo;s default but sometimes you don&rsquo;t want
them. Case in point, I had a service acting as a webhook transmitter a few years
ago. It needed to make requests to many varied upstream targets (almost never
the same).</p><p>The easiest way to turn the default behaviour off is to wire a custom transport
into the client (which I find I&rsquo;m always doing anyway for some of the other
reasons in this fieldnote):</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>client</span> <span class=o>:=</span> <span class=o>&amp;</span><span class=nx>http</span><span class=p>.</span><span class=nx>Client</span><span class=p>{</span>
    <span class=o>&amp;</span><span class=nx>http</span><span class=p>.</span><span class=nx>Transport</span><span class=p>{</span>
        <span class=nx>DisableKeepAlives</span><span class=p>:</span> <span class=kc>true</span>
    <span class=p>}</span>
<span class=p>}</span>
</code></pre></div><p>You can, however, also do this per request by telling the Go client to close it
for you:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>req</span><span class=p>.</span><span class=nx>Close</span> <span class=p>=</span> <span class=kc>true</span>
</code></pre></div><p>Or otherwise signalling a well-behaving server to add a <code>Connection: close</code>
response header with which the Go client will know what to do.</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=nx>req</span><span class=p>.</span><span class=nx>Header</span><span class=p>.</span><span class=nf>Add</span><span class=p>(</span><span class=s>&#34;Connection&#34;</span><span class=p>,</span> <span class=s>&#34;close&#34;</span><span class=p>)</span>
</code></pre></div><h2 id=connection-pooling>Connection Pooling</h2><p>Continuing with the theme of reuse. In the micro-SOAs I find myself working in,
I am actually <strong>much less likely</strong> to be building that webhook transmitter service
above than I am a service that needs to integrate at high frequency but to only
a few upstreams (e.g. a cloud datastore/queue and a dependant API or two).</p><p>I would argue in this more common scenario the Go <code>http.Client</code> defaults work
against you.</p><p>By that I mean there are some <a href=https://golang.org/src/net/http/transport.go>properties</a> exhibited by the client&rsquo;s default
transport with regards to connection pooling that you should always be mindful
of:</p><div class=highlight><pre class=chroma><code class=language-go data-lang=go><span class=kd>var</span> <span class=nx>DefaultTransport</span> <span class=nx>RoundTripper</span> <span class=p>=</span> <span class=o>&amp;</span><span class=nx>Transport</span><span class=p>{</span>
	<span class=nx>MaxIdleConns</span><span class=p>:</span>          <span class=mi>100</span><span class=p>,</span>
	<span class=nx>IdleConnTimeout</span><span class=p>:</span>       <span class=mi>90</span> <span class=o>*</span> <span class=nx>time</span><span class=p>.</span><span class=nx>Second</span><span class=p>,</span>
<span class=p>}</span>
<span class=o>...</span>
<span class=kd>const</span> <span class=nx>DefaultMaxIdleConnsPerHost</span> <span class=p>=</span> <span class=mi>2</span>
</code></pre></div><p>The relationship between these three settings can be summarised as follows: a connection pool is retained of size 100, but <strong>only 2 per target host</strong>, and if a connection remains unutilised for 90 seconds it will be removed and closed.</p><p>So take the scenario of 100 goroutines sharing the same or default <code>http.Client</code>
to make requests to the same upstream dependency (this is not so contrived if
your client is itself also a server part of a larger microservice ecosystem,
forking routines per request it receives). 98 of those 100 connections get
<strong>closed immediately</strong>.</p><p>First things first, the means your service is working harder. There are myriad
connection establishment costs: kernel network stack processing and allocation;
DNS lookups, of which there may be many (read about <code>resolv.conf(5):ndots:n</code>
especially if you run <a href=https://pracucci.com/kubernetes-dns-resolution-ndots-options-and-why-it-may-affect-application-performances.html>Kubernetes clusters</a>); as well as the TCP and TLS
handshakes to get through.</p><p>This is of course not optimal, but there is another hidden cost that has bitten
me in the past, rendering entire hosts useless: <strong>closed != closed</strong> (in Linux
anyway).</p><p>The kernel actually transitions the socket to a <code>TIME_WAIT</code> state, the purpose
of which being primarily to prevent delayed packets from one connection being
accepted by a subsequent connection. The kernel will keep these around for ~60s
(very hard to change in Linux as per <a href=https://tools.ietf.org/html/rfc793>RFC793</a> adherence).</p><p>A buildup of <code>TIME_WAIT</code> sockets can have adverse effects on the resources of a
busy host.</p><p>For one, there is the additional CPU and memory to maintain the socket structure
in the kernel, but most critically there is the slot in the connection table. A
slot in use means another connection with the same quadruplet (source addr:port,
dest addr:port) cannot exist, and this in turn can result in <strong>ephemeral port
exhaustion</strong> — the dreaded <code>EADDRNOTAVAIL</code>.</p><h2 id=validating-uris>Validating URIs</h2><p>This is a small one, but as far as I&rsquo;m concerned, the <code>url.Parse</code> method is
essentially infallible and it trips me up all the bloody time. You almost always
want <a href=https://golang.org/pkg/net/url/#ParseRequestURI><code>url.ParseRequestURI</code></a> and then some further checks if you are wanting to
filter out relative URLs.</p><h2 id=dns-caching>DNS Caching</h2><p>Unlike the JVM, there is no builtin DNS cache in the Go standard runtime. This
is a double edged sword. I&rsquo;m personally thankful for this default after been
burned countless times by that JVM cache in a past life. At the same time, it is
something to always be cognisant of when trying to produce an optimised Go
service.</p><p>The Go core team&rsquo;s stance is you should defer to the underlying host platform to
support your DNS caching needs by way of something like <a href=https://thekelleys.org.uk/dnsmasq/doc.html>dnsmasq</a>. However, it is
worth pointing out that you are not always in control of that situation. For
example, AWS Lambda&rsquo;s runtime sandbox contains a single remote Route53 address
in <code>/etc/resolv.conf</code> and provides no sandbox-local cache server.</p><p>Another option you have in this situation is to override the <code>DialContext</code> on
<code>http.Transport</code> (as seems to be the general theme of this fieldnote) and wire
in an in-memory cache. I can recommend <a href=https://github.com/rs/dnscache>dnscache</a> for this purpose.</p><blockquote><p>NOTE: There is also the package singleton <code>net.DefaultResolver</code> that you may
need to consider overriding if you don&rsquo;t have full control over your client&rsquo;s
transport.</p></blockquote><p>You might consider one of these options if you have latency-sensitive services
with only a couple of upstream dependencies. Those services will need to
continually dial that same unchanging couple of domains by default.</p><p>I say &ldquo;by default&rdquo; because you might have a well-tuned set of reused connections
(perhaps even in thanks to the section on <a href=#connection-pooling>connection pooling</a>). If that&rsquo;s the
case then caveat emptor—I have another piece of anecdata for you.</p><p>I once had an issue where a high volume Go-based service acting (in part) as a
reverse proxy kept proxying the same dead backend endpoints despite its host
having a TTL-respecting DNS cache. The problem was the service was reusing the
connections so fast that the idle timeouts were never breached.</p><p>As of Go 1.16, nothing in the default runtime will force those established
connections to close and thus get updated resolved IPs for hostnames, forcing
you to get creative with a separate goroutine to call
<code>transport.CloseIdleConnections()</code> on an interval which is less than ideal.
While it is very easy to write a reverse proxy in Go, my mistake here was not
deferring to something more dedicated and endpoint-aware (like the excellent
<a href=https://www.envoyproxy.io/>Envoy proxy</a>).</p><h2 id=masqueraded-dualstack-net-dot-dial-errors>Masqueraded DualStack <code>net.Dial()</code> Errors</h2><p>This one is nuanced but is a <em>belter</em> if it hits and it can surface (as it did for
me) even if you&rsquo;re not actively using IPv6.</p><p>Take a stock Amazon EKS worker node as an example. At the time of writing this
the EKS optimised AMI has the following default traits:</p><ol><li>The Docker daemons do not have the experimental <a href=https://docs.docker.com/config/daemon/ipv6/>IPv6 flag</a> enabled and so will
not configure IPv6 addresses on the container virtual network interfaces.</li><li>But the kernels do have IPv6 support enabled, meaning <code>/proc/net</code> gets the
IPv6 constructs (even in container namespaces) and some other things are
inferred from there, most critically, <code>/etc/hosts</code> receiving two default
entries for loopback: <code>127.0.0.1</code> and <code>::1</code>.</li></ol><p>Now suppose you have a Go service that calls another over loopback, such as a process sidecar, but you naively resolve the loopback address using <code>localhost</code> hostname.</p><blockquote><p>TL;DR: This is where you went wrong. Save yourself the trouble, stop here and
use <code>127.0.0.1</code> or <code>::1</code> depending on your target stack unless you have a good
reason not to. Read on for why you might want to do that.</p></blockquote><p>You don&rsquo;t notice during development, but in an integrated environment running at
scale you see sporadically recurring <code>::1 cannot assign requested address</code>
emitted from the dialer. However, you check the host and ephemeral ports are
<em>absolutely fine</em>. There goes that theory.</p><p>What&rsquo;s with that <code>::1</code> IPv6 address family error though? It&rsquo;s concerning because
from the AMI traits above, we know that a client resolving that address is going
to have a bad time connecting given there&rsquo;s no network interface actually bound
to it. But then again, if that were true why does it not fail <em>all</em> the time?</p><p>It could be that the Go dialer is masquerading the real error—the <strong>IPv4 error</strong>!</p><p>This can happen because of a few subtle defaults:</p><ol><li><p>Firstly, when presented with multiple addresses for the same hostname, the Go
dialer <a href=https://golang.org/src/net/addrselect.go>sorts and selects</a> addresses according to <a href=https://tools.ietf.org/html/rfc6724>RFC6724</a> and critically, this
RFC outlines a preference for the IPv6 <strong>first</strong>.</p></li><li><p>Then, because Go&rsquo;s default network transport also has
<a href=https://tools.ietf.org/html/rfc6555>RFC6555</a> support (aka. Happy Eyeballs /
Dual Stack), it will try to dial both address families in parallel but give
the primary IPv6 a 300 millisecond head start. If the primary fails fast
(which it would always do in this case due to the non-existent IPv6 interface
address), then the head start is cancelled and IPv4 is tried immediately. All
good so far. However, if both addresses fail to dial, only the <strong>primary (i.e.
IPv6) error is returned</strong>.</p></li></ol><p>So if your IPv4 address is failing to dial sporadically (say, for example, a
laggy upstream is causing sporadic connect timeouts) then the error presented
will be the irrelevant and always failing to dial IPv6 <code>::1 cannot assign requested address</code> instead of the much more helpful IPv4 <code>connect timeout</code>.</p><h2 id=net-dot-ip-is-mutable><code>net.IP</code> is Mutable</h2><p>This one bit me in production, albeit when I was doing something stupid. Don&rsquo;t
be tricked into thinking <code>net.IP</code> is an immutable data structure. It is in fact
a transparent type aliased to <code>[]byte</code>. Anything you pass it to could mutate it
and Sod&rsquo;s/Murphy&rsquo;s Law says it will.</p><h2 id=bonus-gomaxprocs-containers-and-the-cfs>Bonus: GOMAXPROCS, Containers and the CFS</h2><p>This one is not specifically related to the <code>net</code> package defaults but sure can
affect their performance indirectly.</p><p>Go will use the <code>GOMAXPROCS</code> variable value at init to decide how many real OS
threads to multiplex all user-level Go routines across. By default it is set to
the number of logical CPUs discovered in the host environment and I suppose this
is another example of the Go core team needing to settle on a sensible default
i.e. the number of logical CPUs in a generalised context is what provides the
highest performance.</p><p>Unfortunately the Go runtime also happens to be unaware of CFS quotas (as of
1.16), and multi-tenant container orchestration platforms (like Kubernetes) will
default to using CFS quotas to enforce their respective CPU restriction concepts
on the running container&rsquo;s cgroup.</p><blockquote><p>NOTE: CFS here being the Linux kernel&rsquo;s <a href=https://en.wikipedia.org/wiki/Completely%5FFair%5FScheduler>Completely Fair Scheduler</a>—a proportional
share scheduler that divides available CPU bandwidth between cgroups.</p></blockquote><p>Continuing with Kubernetes as an example, a default <code>GOMAXPROCS</code> compounded by a
CFS quota-unaware runtime means your Go service <code>Pod</code> complete with carefully
considered CPU resource limits and request specs can find itself being
<strong>aggressively and unduly throttled</strong> by the CFS.</p><p>Why is this? Well say for instance your <code>Pod</code> spec has a CPU resource limit of
300m and taking the Linux kernel&rsquo;s stock CFS quota window of 100ms, this gives
the Go service 30ms of CPU time per window. If it tries to use more than that it
gets throttled by the CFS until the next window. This is fine and is presumably
expected because you scientifically measured your service locally and landed on
that 300m request in the first place (right?).</p><p>However, it is important to remember that the 30ms is actually further
subdivided between <code>GOMAXPROCS</code> OS threads, so if your <code>Pod</code> happens to land on
a large commoditised host VM with, for simplicity&rsquo;s sake, 15 logical CPUs (as is
common in binpack-styled Kubernetes clusters topologies) then your Go service
will have set <code>GOMAXPROCS</code> to 15, giving each resulting OS thread potentially
just <strong>2ms</strong> of CPU time per scheduling window before it gets throttled and
preempted!</p><p>The result is a Go runtime scheduler wasting all its allotted CPU time on
context switching and getting no useful work done because it thinks it has
access to 15 logical CPUs when in fact it has 0.3.</p><blockquote><p>NOTE: To see if this is affecting Kubernetes services you are responsible for, the kubelet cAdvisor metric <code>container_cpu_cfs_throttled_periods_total</code> is king.</p></blockquote><p><code>GOMAXPROCS</code> is an integer so your only recourse in the example above is to
hardcode it to 1. This can be done either through the environment with <code>export GOMAXPROCS=1</code>, or with the package func <code>runtime.GOMAXPROCS(1)</code>.</p><p>More generally though, I would recommend Uber&rsquo;s <a href=https://github.com/uber-go/automaxprocs>automaxprocs</a> drop-in to make
your Go runtime CFS aware. For Kubernetes, You can also disable CFS quota
enforcement at the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/>kubelet level</a> if you were so inclined.</p></div></section><div class=post-tags></div></article></main><footer><hr><a class=soc rel=me href=https://github.com/martinbaillie title=GitHub><i data-feather=github></i></a><a class=soc rel=me href=https://linkedin.com/in/martinbaillie title=LinkedIn><i data-feather=linkedin></i></a><a class=soc rel=me href=https://twitter.com/martinbaillie title=Twitter><i data-feather=twitter></i></a><a class=soc rel=me href=mailto:martin@baillie.id title=Email><i data-feather=mail></i></a><a class="soc gpg" href=https://github.com/martinbaillie.gpg title="GPG Public Key">C2F0 79DE D64B 7361 006A A099 2A56 EA64 591E 15E4</a>
<a class=menu href=/id>$id</a>
<span class="active menu">fieldnotes</span></footer><script src=https://martin.baillie.id/js/footer.min.28ef130186726a0df510071a5be15530d6861d48b3bd20da5f164370ecc400914afdc175b237ca1a21d9a09b59fe738344e299f812348b8f77c8a9f5c30ed91f.js integrity="sha512-KO8TAYZyag31EAcaW+FVMNaGHUizvSDaXxZDcOzEAJFK/cF1sjfKGiHZoJtZ/nODROKZ+BI0i493yKn1ww7ZHw==" crossorigin=anonymous async></script></div></body></html>